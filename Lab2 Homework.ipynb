{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Student Information\nName: Duygu\n\nStudent ID: X1100011\n\nGitHub ID: duygu-bayrak\n\nKaggle name: duygu\n\nKaggle private scoreboard snapshot:\n\n[Snapshot](img/pic0.png)","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### Instructions","metadata":{}},{"cell_type":"markdown","source":"1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2021-Lab2-master Repo](https://github.com/fhcalderon87/DM2021-Lab2-master). You may need to copy some cells from the Lab notebook to this notebook. \n\n\n2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2021-lab2-hw2/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n    - **Bottom 40%**: Get 20% of the 30% available for this section.\n\n    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n    Submit your last submission __BEFORE the deadline (Dec. 24th 11:59 pm, Friday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n    \n\n3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n\n\n4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n\n\nUpload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n\nMake sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 29th 11:59 pm, Wednesday)__. ","metadata":{}},{"cell_type":"markdown","source":"# 1. Part","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:49:54.015443Z","iopub.execute_input":"2021-12-23T08:49:54.015733Z","iopub.status.idle":"2021-12-23T08:49:54.037254Z","shell.execute_reply.started":"2021-12-23T08:49:54.015697Z","shell.execute_reply":"2021-12-23T08:49:54.036424Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Code for Exercise 1","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npath = \"/kaggle/input/dmlab2-ww/DM2021-Lab2-master/\"\n### training data\nanger_train = pd.read_csv(path+\"data/semeval/train/anger-ratings-0to1.train.txt\",\n                         sep=\"\\t\", header=None,names=[\"id\", \"text\", \"emotion\", \"intensity\"])\nsadness_train = pd.read_csv(path+\"data/semeval/train/sadness-ratings-0to1.train.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\nfear_train = pd.read_csv(path+\"data/semeval/train/fear-ratings-0to1.train.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\njoy_train = pd.read_csv(path+\"data/semeval/train/joy-ratings-0to1.train.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:38:38.787702Z","iopub.execute_input":"2021-12-23T08:38:38.787960Z","iopub.status.idle":"2021-12-23T08:38:38.851410Z","shell.execute_reply.started":"2021-12-23T08:38:38.787933Z","shell.execute_reply":"2021-12-23T08:38:38.850761Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# combine 4 sub-dataset\ntrain_df = pd.concat([anger_train, fear_train, joy_train, sadness_train], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:38:52.713110Z","iopub.execute_input":"2021-12-23T08:38:52.714103Z","iopub.status.idle":"2021-12-23T08:38:52.724784Z","shell.execute_reply.started":"2021-12-23T08:38:52.714047Z","shell.execute_reply":"2021-12-23T08:38:52.724060Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"### testing data\nanger_test = pd.read_csv(path+\"data/semeval/dev/anger-ratings-0to1.dev.gold.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\nsadness_test = pd.read_csv(path+\"data/semeval/dev/sadness-ratings-0to1.dev.gold.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\nfear_test = pd.read_csv(path+\"data/semeval/dev/fear-ratings-0to1.dev.gold.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\njoy_test = pd.read_csv(path+\"data/semeval/dev/joy-ratings-0to1.dev.gold.txt\",\n                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n\n# combine 4 sub-dataset\ntest_df = pd.concat([anger_test, fear_test, joy_test, sadness_test], ignore_index=True)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:38:57.303591Z","iopub.execute_input":"2021-12-23T08:38:57.303901Z","iopub.status.idle":"2021-12-23T08:38:57.362459Z","shell.execute_reply.started":"2021-12-23T08:38:57.303868Z","shell.execute_reply":"2021-12-23T08:38:57.361572Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# shuffle dataset\ntrain_df = train_df.sample(frac=1)\ntest_df = test_df.sample(frac=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:39:03.062945Z","iopub.execute_input":"2021-12-23T08:39:03.063554Z","iopub.status.idle":"2021-12-23T08:39:03.073654Z","shell.execute_reply.started":"2021-12-23T08:39:03.063516Z","shell.execute_reply":"2021-12-23T08:39:03.072891Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 1 (Take home): **  \nPlot word frequency for Top 30 words in both train and test dataset. (Hint: refer to DM lab 1)\n","metadata":{}},{"cell_type":"code","source":"# Answer here\nimport re\nfrom collections import Counter\n\ndef split_sentence(sentence):\n    return list(filter(lambda x: len(x) > 0, re.split('\\W+', sentence.lower())))\n\ndef generate_vocabulary(train_captions, min_threshold):\n    \"\"\"\n    Return {token: index} for all train tokens (words) that occur min_threshold times or more, \n        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n    \"\"\"  \n    #convert the list of whole captions to one string\n    concat_str = ' '.join([str(elem).strip('\\n') for elem in train_captions]) \n    #divide the string tokens (individual words), by calling the split_sentence function \n    individual_words = split_sentence(concat_str)\n    #create a list of words that happen min_threshold times or more in that string  \n    #condition_keys = sorted([key for key, value in Counter(individual_words).items() if value >= min_threshold])\n    #generate the vocabulary(dictionary)\n    #result = dict(zip(condition_keys, range(len(condition_keys))))\n    return Counter(individual_words)#result","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:39:06.498490Z","iopub.execute_input":"2021-12-23T08:39:06.498909Z","iopub.status.idle":"2021-12-23T08:39:06.505606Z","shell.execute_reply.started":"2021-12-23T08:39:06.498879Z","shell.execute_reply":"2021-12-23T08:39:06.504721Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# word frequency for Top 30 words in train dataset\ntrain_freqs = generate_vocabulary(train_df.text.tolist(), 0)\ntrain_freqs.most_common(30)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:39:13.564534Z","iopub.execute_input":"2021-12-23T08:39:13.565117Z","iopub.status.idle":"2021-12-23T08:39:13.634865Z","shell.execute_reply.started":"2021-12-23T08:39:13.565080Z","shell.execute_reply":"2021-12-23T08:39:13.634309Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# word frequency for Top 30 words in train dataset\ntest_freqs = generate_vocabulary(test_df.text.tolist(), 0)\ntest_freqs.most_common(30)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:39:16.286019Z","iopub.execute_input":"2021-12-23T08:39:16.286656Z","iopub.status.idle":"2021-12-23T08:39:16.301008Z","shell.execute_reply.started":"2021-12-23T08:39:16.286617Z","shell.execute_reply":"2021-12-23T08:39:16.300180Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 2 (Take home): **  \nGenerate an embedding using the TF-IDF vectorizer instead of th BOW one with 1000 features and show the feature names for features [100:110].","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:40:24.109968Z","iopub.execute_input":"2021-12-23T08:40:24.110269Z","iopub.status.idle":"2021-12-23T08:40:44.826971Z","shell.execute_reply.started":"2021-12-23T08:40:24.110240Z","shell.execute_reply":"2021-12-23T08:40:44.826401Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Answer here\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(max_features=1000, tokenizer=nltk.word_tokenize)\n\ntrain_data_tfidf_features_1000 = tfidf_vectorizer.fit_transform(train_df['text'])\ntest_data_tfidf_features = tfidf_vectorizer.transform(test_df['text'])\n\ntrain_data_tfidf_features_1000.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:40:44.828301Z","iopub.execute_input":"2021-12-23T08:40:44.828637Z","iopub.status.idle":"2021-12-23T08:40:46.209669Z","shell.execute_reply.started":"2021-12-23T08:40:44.828610Z","shell.execute_reply":"2021-12-23T08:40:46.208862Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"feature_names_tfidf = tfidf_vectorizer.get_feature_names()\nfeature_names_tfidf[100:110]","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:40:46.210725Z","iopub.execute_input":"2021-12-23T08:40:46.210944Z","iopub.status.idle":"2021-12-23T08:40:46.217771Z","shell.execute_reply.started":"2021-12-23T08:40:46.210916Z","shell.execute_reply":"2021-12-23T08:40:46.216968Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\"ðŸ˜‚\" in feature_names_tfidf","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:40:55.965114Z","iopub.execute_input":"2021-12-23T08:40:55.965420Z","iopub.status.idle":"2021-12-23T08:40:55.971543Z","shell.execute_reply.started":"2021-12-23T08:40:55.965388Z","shell.execute_reply":"2021-12-23T08:40:55.970764Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## code needed for Exercise 3","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# build analyzers (bag-of-words)\nBOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n\n# apply analyzer to training data\nBOW_500.fit(train_df['text'])\n\ntrain_data_BOW_features_500 = BOW_500.transform(train_df['text'])\n\n## check dimension\ntrain_data_BOW_features_500.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:59:35.249912Z","iopub.execute_input":"2021-12-23T08:59:35.250632Z","iopub.status.idle":"2021-12-23T08:59:37.664920Z","shell.execute_reply.started":"2021-12-23T08:59:35.250580Z","shell.execute_reply":"2021-12-23T08:59:37.664380Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# for a classificaiton problem, you need to provide both training & testing data\nX_train = BOW_500.transform(train_df['text'])\ny_train = train_df['emotion']\n\nX_test = BOW_500.transform(test_df['text'])\ny_test = test_df['emotion']\n\n## take a look at data dimension is a good habbit  :)\nprint('X_train.shape: ', X_train.shape)\nprint('y_train.shape: ', y_train.shape)\nprint('X_test.shape: ', X_test.shape)\nprint('y_test.shape: ', y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:59:39.734472Z","iopub.execute_input":"2021-12-23T08:59:39.734750Z","iopub.status.idle":"2021-12-23T08:59:41.155208Z","shell.execute_reply.started":"2021-12-23T08:59:39.734721Z","shell.execute_reply":"2021-12-23T08:59:41.154322Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"## build DecisionTree model\nDT_model = DecisionTreeClassifier(random_state=0)\n\n## training!\nDT_model = DT_model.fit(X_train, y_train)\n\n## predict!\ny_train_pred = DT_model.predict(X_train)\ny_test_pred = DT_model.predict(X_test)\n\n## so we get the pred result\ny_test_pred[:10]","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:59:44.023551Z","iopub.execute_input":"2021-12-23T08:59:44.023953Z","iopub.status.idle":"2021-12-23T08:59:44.209417Z","shell.execute_reply.started":"2021-12-23T08:59:44.023914Z","shell.execute_reply":"2021-12-23T08:59:44.208559Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"## check by confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \nprint(cm)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:59:45.366143Z","iopub.execute_input":"2021-12-23T08:59:45.366882Z","iopub.status.idle":"2021-12-23T08:59:45.374801Z","shell.execute_reply.started":"2021-12-23T08:59:45.366843Z","shell.execute_reply":"2021-12-23T08:59:45.373921Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Funciton for visualizing confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix',\n                          cmap=sns.cubehelix_palette(as_cmap=True)):\n    \"\"\"\n    This function is modified from: \n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n    \"\"\"\n    classes.sort()\n    tick_marks = np.arange(len(classes))    \n    \n    fig, ax = plt.subplots(figsize=(5,5))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           xticklabels = classes,\n           yticklabels = classes,\n           title = title,\n           xlabel = 'True label',\n           ylabel = 'Predicted label')\n\n    fmt = 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n    ylim_top = len(classes) - 0.5\n    plt.ylim([ylim_top, -.5])\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T08:59:53.748818Z","iopub.execute_input":"2021-12-23T08:59:53.749112Z","iopub.status.idle":"2021-12-23T08:59:53.761379Z","shell.execute_reply.started":"2021-12-23T08:59:53.749069Z","shell.execute_reply":"2021-12-23T08:59:53.760419Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# plot your confusion matrix\nmy_tags = ['anger', 'fear', 'joy', 'sadness']\nplot_confusion_matrix(cm, classes=my_tags, title='Confusion matrix')","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:00:27.640226Z","iopub.execute_input":"2021-12-23T09:00:27.640523Z","iopub.status.idle":"2021-12-23T09:00:27.964171Z","shell.execute_reply.started":"2021-12-23T09:00:27.640493Z","shell.execute_reply":"2021-12-23T09:00:27.963389Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 3 (Take home): **  \nCan you interpret the results above? What do they mean?\n","metadata":{}},{"cell_type":"markdown","source":"# Answer here\nthe model classified:\n- 57 sentences correctly as anger\n- 75 sentences correctly as fear\n- 53 sentences correctly as joy\n- 44 sentences correctly as sadness\n\nthe model misclassified:\n- 11 fear, 7 joy, and 9 sadness sentences as anger\n- 11 anger, 14 joy, and 10 sadness sentences as fear\n- 6 anger, 13, fear, and 7 sadness sentences as joy\n- 7 anger, 12 fear, and 11 joy sentences as sadness","metadata":{}},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 4 (Take home): **  \nBuild a model using a ```Naive Bayes``` model and train it. What are the testing results? \n\n*Reference*: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n","metadata":{}},{"cell_type":"code","source":"# Answer here\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB(alpha=0.1)\n\n#train\nclf.fit(X_train, y_train)\n\n## predict!\ny_train_pred = clf.predict(X_train)\ny_test_pred = clf.predict(X_test)\n\n## so we get the pred result\ny_test_pred[:10]","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:00:41.760047Z","iopub.execute_input":"2021-12-23T09:00:41.760777Z","iopub.status.idle":"2021-12-23T09:00:41.787706Z","shell.execute_reply.started":"2021-12-23T09:00:41.760735Z","shell.execute_reply":"2021-12-23T09:00:41.786732Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"## accuracy\nfrom sklearn.metrics import accuracy_score\n\nacc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\nacc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n\nprint('training accuracy: {}'.format(round(acc_train, 2)))\nprint('testing accuracy: {}'.format(round(acc_test, 2)))","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:00:45.051065Z","iopub.execute_input":"2021-12-23T09:00:45.052021Z","iopub.status.idle":"2021-12-23T09:00:45.064465Z","shell.execute_reply.started":"2021-12-23T09:00:45.051968Z","shell.execute_reply":"2021-12-23T09:00:45.063530Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 5 (Take home): **  \n\nHow do the results from the Naive Bayes model and the Decision Tree model compare? How do you interpret these differences? Use the theoretical background covered in class to try and explain these differences.","metadata":{}},{"cell_type":"markdown","source":"# Answer here\nThe decision tree model yielded a training accuracy of 0.99 and a testing accuracy of 0.66.\nThe naive bayes model yielded a training accuracy of 0.8 and a testing accuracy of 0.73.\n\nSince the first model has a training accuracy of 99%, I would say that it is overfitting to the train data. This would also explain the lower testing accuracy compared to the second model.\nThe second model generalizes better, the testing accuracy is not much lower than the training accuracy. This is because the Naive Bayes classifier employs a very simple (linear) hypothesis function, which prevents it from overfitting to its training data.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Code for Exercise 6","metadata":{}},{"cell_type":"code","source":"# pip3 install keras\nimport keras","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:04:22.006864Z","iopub.execute_input":"2021-12-23T09:04:22.007138Z","iopub.status.idle":"2021-12-23T09:04:22.011238Z","shell.execute_reply.started":"2021-12-23T09:04:22.007111Z","shell.execute_reply":"2021-12-23T09:04:22.010342Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"## deal with label (string -> one-hot)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(y_train)\nprint('check label: ', label_encoder.classes_)\nprint('\\n## Before convert')\nprint('y_train[0:4]:\\n', y_train[0:4])\nprint('\\ny_train.shape: ', y_train.shape)\nprint('y_test.shape: ', y_test.shape)\n\ndef label_encode(le, labels):\n    enc = le.transform(labels)\n    return keras.utils.np_utils.to_categorical(enc) #keras.utils.to_categorical(enc)\n\ndef label_decode(le, one_hot_label):\n    dec = np.argmax(one_hot_label, axis=1)\n    return le.inverse_transform(dec)\n\ny_train = label_encode(label_encoder, y_train)\ny_test = label_encode(label_encoder, y_test)\n\nprint('\\n\\n## After convert')\nprint('y_train[0:4]:\\n', y_train[0:4])\nprint('\\ny_train.shape: ', y_train.shape)\nprint('y_test.shape: ', y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:04:24.493634Z","iopub.execute_input":"2021-12-23T09:04:24.493900Z","iopub.status.idle":"2021-12-23T09:04:24.509976Z","shell.execute_reply.started":"2021-12-23T09:04:24.493872Z","shell.execute_reply":"2021-12-23T09:04:24.509137Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# I/O check\ninput_shape = X_train.shape[1]\nprint('input_shape: ', input_shape)\n\noutput_shape = len(label_encoder.classes_)\nprint('output_shape: ', output_shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:04:28.145842Z","iopub.execute_input":"2021-12-23T09:04:28.146142Z","iopub.status.idle":"2021-12-23T09:04:28.155806Z","shell.execute_reply.started":"2021-12-23T09:04:28.146107Z","shell.execute_reply":"2021-12-23T09:04:28.155100Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.layers import ReLU, Softmax\n\n# input layer\nmodel_input = Input(shape=(input_shape, ))  # 500\nX = model_input\n\n# 1st hidden layer\nX_W1 = Dense(units=64)(X)  # 64\nH1 = ReLU()(X_W1)\n\n# 2nd hidden layer\nH1_W2 = Dense(units=64)(H1)  # 64\nH2 = ReLU()(H1_W2)\n\n# output layer\nH2_W3 = Dense(units=output_shape)(H2)  # 4\nH3 = Softmax()(H2_W3)\n\nmodel_output = H3\n\n# create model\nmodel = Model(inputs=[model_input], outputs=[model_output])\n\n# loss function & optimizer\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# show model construction\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:04:29.670836Z","iopub.execute_input":"2021-12-23T09:04:29.671105Z","iopub.status.idle":"2021-12-23T09:04:29.828834Z","shell.execute_reply.started":"2021-12-23T09:04:29.671077Z","shell.execute_reply":"2021-12-23T09:04:29.827828Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# You can write up to 20GB to the current directory (/kaggle/working/)\npath = \"/kaggle/working/\"\n\nimport pandas as pd\ndf = pd.DataFrame(list())\ndf.to_csv(path+'training_log.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:07:47.213649Z","iopub.execute_input":"2021-12-23T09:07:47.214593Z","iopub.status.idle":"2021-12-23T09:07:47.220187Z","shell.execute_reply.started":"2021-12-23T09:07:47.214548Z","shell.execute_reply":"2021-12-23T09:07:47.219399Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import CSVLogger\n\n\ncsv_logger = CSVLogger(path+'training_log.csv')\n\n# training setting\nepochs = 25\nbatch_size = 32\n\n# training!\nhistory = model.fit(X_train, y_train, \n                    epochs=epochs, \n                    batch_size=batch_size, \n                    callbacks=[csv_logger],\n                    validation_data = (X_test, y_test))\nprint('training finish')","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:08:00.420440Z","iopub.execute_input":"2021-12-23T09:08:00.421009Z","iopub.status.idle":"2021-12-23T09:08:30.545083Z","shell.execute_reply.started":"2021-12-23T09:08:00.420970Z","shell.execute_reply":"2021-12-23T09:08:30.544324Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"#Let's take a look at the training log\ntraining_log = pd.DataFrame()\ntraining_log = pd.read_csv(path+\"training_log.csv\")\ntraining_log.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:09:36.217481Z","iopub.execute_input":"2021-12-23T09:09:36.217796Z","iopub.status.idle":"2021-12-23T09:09:36.231541Z","shell.execute_reply.started":"2021-12-23T09:09:36.217750Z","shell.execute_reply":"2021-12-23T09:09:36.230781Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 6 (Take home): **  \n\nPlot the Training and Validation Accuracy and Loss (different plots), just like the images below (Note: the pictures below are an example from a different model). How to interpret the graphs you got? How are they related to the concept of overfitting/underfitting covered in class?\n<table><tr>\n    <td><img src=\"pics/pic3.png\" style=\"width: 300px;\"/> </td>\n    <td><img src=\"pics/pic4.png\" style=\"width: 300px;\"/> </td>\n</tr></table>","metadata":{}},{"cell_type":"code","source":"# Answer here\ntrain_acc = training_log['accuracy'].tolist()\nval_acc = training_log['val_accuracy'].tolist()\ntrain_loss = training_log['loss'].tolist()\nval_loss = training_log['val_loss'].tolist()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:09:39.832429Z","iopub.execute_input":"2021-12-23T09:09:39.833126Z","iopub.status.idle":"2021-12-23T09:09:39.839815Z","shell.execute_reply.started":"2021-12-23T09:09:39.833089Z","shell.execute_reply":"2021-12-23T09:09:39.838983Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.plot(train_acc)\nplt.plot(val_acc)\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()\n\nplt.plot(train_loss)\nplt.plot(val_loss)\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:09:42.529797Z","iopub.execute_input":"2021-12-23T09:09:42.530324Z","iopub.status.idle":"2021-12-23T09:09:42.944899Z","shell.execute_reply.started":"2021-12-23T09:09:42.530290Z","shell.execute_reply":"2021-12-23T09:09:42.944166Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"we can clearly see an overfitiing here. If we look at the graph \"model loss\", we can see that the validation loss first decreases until epoch 4 and then it increases. The train loss, however, keeps decreasing and reaches almost 0% at epoch 25. If we then look at the graph \"model accuracy\", we can see that the train accuracy increases very steeply and flattens after epoch 5. So, at about epoch 4 the train accuracy reaches 90% and this is the point where the overfitting happens since the validation loss then starts to increase again.","metadata":{}},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 7 (Take home): **  \n\nNow, we have the word vectors, but our input data is a sequence of words (or say sentence). \nHow can we utilize these \"word\" vectors to represent the sentence data and train our model?\n","metadata":{}},{"cell_type":"markdown","source":"## Answer: we can calculate the average vector for all words in every sentence (see code below)","metadata":{}},{"cell_type":"code","source":"# Answer here\n# calculate the average vector for all words in every sentence\ndef get_mean_vector(word2vec_model, words):\n    # remove out-of-vocabulary words\n    words = [word for word in words if word in word2vec_model.vocab]\n    if len(words) >= 1:\n        return np.mean(word2vec_model[words], axis=0)\n    else:\n        return []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Code for Exercise 8","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n### ** >>> Exercise 8 (Take home): **  \n\nGenerate a t-SNE visualization to show the 15 words most related to the words \"angry\", \"happy\", \"sad\", \"fear\" (60 words total).","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\n## Note: this model is very huge, this will take some time ...\nmodel_path = \"/kaggle/input/dmlab2-ww/GoogleNews-vectors-negative300.bin\" #\"GoogleNews/GoogleNews-vectors-negative300.bin.gz\"\nw2v_google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\nprint('load ok')\n\nw2v_google_model.most_similar('happy', topn=10)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:14:34.476293Z","iopub.execute_input":"2021-12-23T09:14:34.476674Z","iopub.status.idle":"2021-12-23T09:15:58.647269Z","shell.execute_reply.started":"2021-12-23T09:14:34.476642Z","shell.execute_reply":"2021-12-23T09:15:58.646073Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Answer here\nword_list = ['happy', 'angry', 'sad', 'fear']\n\ntopn = 15\nhappy_words = [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\nangry_words = [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \nsad_words = [word_ for word_, sim_ in w2v_google_model.most_similar('sad', topn=topn)]        \nfear_words = [word_ for word_, sim_ in w2v_google_model.most_similar('fear', topn=topn)]        \n\nprint('happy_words: ', happy_words)\nprint('angry_words: ', angry_words)\nprint('sad_words: ', sad_words)\nprint('fear_words: ', fear_words)\n\ntarget_words = happy_words + angry_words + sad_words + fear_words\nprint('\\ntarget words: ')\nprint(target_words)\n\nprint('\\ncolor list:')\ncn = topn \ncolor = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\nprint(color)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:17:21.779399Z","iopub.execute_input":"2021-12-23T09:17:21.779742Z","iopub.status.idle":"2021-12-23T09:17:22.883074Z","shell.execute_reply.started":"2021-12-23T09:17:21.779704Z","shell.execute_reply":"2021-12-23T09:17:22.882103Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n## w2v model\nmodel = w2v_google_model\n\n## prepare training word vectors\nsize = 200\ntarget_size = len(target_words)\nall_word = list(model.index_to_key)\nword_train = target_words + all_word[:size]\nX_train = model[word_train]\n\n## t-SNE model\ntsne = TSNE(n_components=2, metric='cosine', random_state=28)\n\n## training\nX_tsne = tsne.fit_transform(X_train)\n\n## plot the result\nplt.figure(figsize=(7.5, 7.5), dpi=115)\nplt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\nfor label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:18:12.039282Z","iopub.execute_input":"2021-12-23T09:18:12.040254Z","iopub.status.idle":"2021-12-23T09:18:14.381498Z","shell.execute_reply.started":"2021-12-23T09:18:12.040202Z","shell.execute_reply":"2021-12-23T09:18:14.380827Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# 3. Part","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport pandas as pd\nimport json\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:25:05.365773Z","iopub.execute_input":"2021-12-23T09:25:05.366497Z","iopub.status.idle":"2021-12-23T09:25:05.370171Z","shell.execute_reply.started":"2021-12-23T09:25:05.366456Z","shell.execute_reply":"2021-12-23T09:25:05.369582Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### 1. load data","metadata":{}},{"cell_type":"code","source":"# /kaggle/input/dm2021-lab2-hw2/tweets_DM.json\n# /kaggle/input/dm2021-lab2-hw2/sampleSubmission.csv\n# /kaggle/input/dm2021-lab2-hw2/data_identification.csv\n# /kaggle/input/dm2021-lab2-hw2/emotion.csv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_id = pd.read_csv('/kaggle/input/dm2021-lab2-hw2/data_identification.csv')\nX_id.head()\nX_id[\"identification\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:28:08.211722Z","iopub.execute_input":"2021-12-23T09:28:08.212051Z","iopub.status.idle":"2021-12-23T09:28:10.257127Z","shell.execute_reply.started":"2021-12-23T09:28:08.212018Z","shell.execute_reply":"2021-12-23T09:28:10.256328Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"json_lines = []\n\nwith open('/kaggle/input/dm2021-lab2-hw2/tweets_DM.json', 'r') as f:\n    for line in tqdm(f):\n        tweet = json.loads(line)['_source']\n        json_lines.append([tweet[\"tweet\"][\"tweet_id\"], tweet[\"tweet\"][\"text\"]])\n        \n\nX_text = pd.DataFrame(json_lines, columns =['tweet_id', 'text'])\nX_text.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:47:48.919806Z","iopub.execute_input":"2021-12-23T09:47:48.920178Z","iopub.status.idle":"2021-12-23T09:48:10.509760Z","shell.execute_reply.started":"2021-12-23T09:47:48.920138Z","shell.execute_reply":"2021-12-23T09:48:10.508913Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"X_id_text = pd.merge(X_id, X_text, on = \"tweet_id\")\nX_id_text.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:48:10.511121Z","iopub.execute_input":"2021-12-23T09:48:10.512351Z","iopub.status.idle":"2021-12-23T09:48:14.521973Z","shell.execute_reply.started":"2021-12-23T09:48:10.512320Z","shell.execute_reply":"2021-12-23T09:48:14.521118Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"X_id_text_train =  X_id_text[X_id_text['identification'] == \"train\"]\nX_test_df =  X_id_text[X_id_text['identification'] == \"test\"]\nX_test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:52:33.927314Z","iopub.execute_input":"2021-12-23T09:52:33.928002Z","iopub.status.idle":"2021-12-23T09:52:34.701664Z","shell.execute_reply.started":"2021-12-23T09:52:33.927959Z","shell.execute_reply":"2021-12-23T09:52:34.700856Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# reset index\nX_test_df.reset_index(drop=True, inplace=True)\nX_test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:52:51.216767Z","iopub.execute_input":"2021-12-23T09:52:51.217062Z","iopub.status.idle":"2021-12-23T09:52:51.227967Z","shell.execute_reply.started":"2021-12-23T09:52:51.217031Z","shell.execute_reply":"2021-12-23T09:52:51.227011Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"X_emotion = pd.read_csv('/kaggle/input/dm2021-lab2-hw2/emotion.csv')\nX_train_df = pd.merge(X_id_text_train, X_emotion, on = \"tweet_id\")\nX_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:48:15.737943Z","iopub.execute_input":"2021-12-23T09:48:15.738327Z","iopub.status.idle":"2021-12-23T09:48:19.105338Z","shell.execute_reply.started":"2021-12-23T09:48:15.738284Z","shell.execute_reply":"2021-12-23T09:48:19.104451Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/working/\"\n\nX_train_df.to_pickle(path+\"train.pkl\")\nX_test_df.to_pickle(path+\"test.pkl\")","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:53:10.901516Z","iopub.execute_input":"2021-12-23T09:53:10.901811Z","iopub.status.idle":"2021-12-23T09:53:13.882971Z","shell.execute_reply.started":"2021-12-23T09:53:10.901778Z","shell.execute_reply":"2021-12-23T09:53:13.881820Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"## 2. data visualization","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:33:01.702899Z","iopub.execute_input":"2021-12-23T09:33:01.703498Z","iopub.status.idle":"2021-12-23T09:33:03.001109Z","shell.execute_reply.started":"2021-12-23T09:33:01.703344Z","shell.execute_reply":"2021-12-23T09:33:03.000323Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"fig=px.histogram(X_train_df,\n                x='emotion',\n                title='Sentiment Count ',\n                color_discrete_sequence=['green'])\nfig.update_layout(bargap=0.1)\nfig.show()\nfig = px.pie(X_train_df, names='emotion',title=\"Sentiment Distribution\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:33:09.842249Z","iopub.execute_input":"2021-12-23T09:33:09.842562Z","iopub.status.idle":"2021-12-23T09:33:31.470121Z","shell.execute_reply.started":"2021-12-23T09:33:09.842528Z","shell.execute_reply":"2021-12-23T09:33:31.468747Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"## 3. pre-processing","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:33:45.058228Z","iopub.execute_input":"2021-12-23T09:33:45.058518Z","iopub.status.idle":"2021-12-23T09:33:45.063039Z","shell.execute_reply.started":"2021-12-23T09:33:45.058491Z","shell.execute_reply":"2021-12-23T09:33:45.062199Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"import nltk\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer() \nfrom gensim.models import Word2Vec","metadata":{"execution":{"iopub.status.busy":"2021-12-23T09:33:59.827868Z","iopub.execute_input":"2021-12-23T09:33:59.828302Z","iopub.status.idle":"2021-12-23T09:34:59.931332Z","shell.execute_reply.started":"2021-12-23T09:33:59.828259Z","shell.execute_reply":"2021-12-23T09:34:59.930473Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"import string\n\ndef data_cleaning(data):\n    cleaned_data = []\n    fillerWord = (\"so\",\"yeah\",\"okay\",\"um\",\"uh\",\"mmm\",\"ahan\",\"uh\",\"huh\",\"ahm\",\"oh\",\"sooo\",\"uh\",\"huh\",\"yeh\",\"yah\",\"hmm\",\"bye\")\n    fillerword_reg= \"bye[.,]|so[.,]|yeah[.,]|okay[.,]|um[.,]|uh[.,]|mmm[.,]|ahan[.,]|uh[.,]|huh[.,]|ahm[.,]|oh[.,]|sooo[.,]|uh[.,]|huh[.,]|yeh[.,]|yah[.,]|hmm[.,]\"\n    STOPWORDS = set(stopwords.words('english'))\n    remove=[\"doesn't\",\"not\",\"nor\",\"neither\",\"isn't\",\"hadn't\",\"mightn't\",\"needn't\",\"wasn't\"]\n    for i in remove:\n        STOPWORDS.discard(i)\n    \n    STOPWORDS.add(fillerWord)  \n    for i in range(len(data)):\n        tweet = re.sub(\"#\", \"\", data[i])#extracting hashtags\n        tweet = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '',tweet, flags=re.MULTILINE)#extracting links\n        html=re.compile(r'<.*?>')#extracting html tags\n        tweet =html.sub(r\"\", tweet)\n        #extracting symbols and characters\n        tweet=re.sub(r'@\\w+',\"\",tweet)\n        tweet=re.sub(r'#\\w+',\"\",tweet) \n        tweet=re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet) \n        punctuation = r\"\\\"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n        #tweet.rstrip(string.punctuation)\n        tweet = re.sub(r\"\"\"\n               [,.;@#?!&$]+  # Accept one or more copies of punctuation\n               \\ *           # plus zero or more copies of a space,\n               \"\"\",\n               \" \",          # and replace it with a single space\n               tweet)\n        #tweet = re.sub(punctuation, \" \", tweet)\n        tweet=re.sub('[^A-Za-z\\s]+',\"\", tweet)\n        tweet = tweet.lower()\n        tweet = tweet.split()\n        #Lemmatization to normalise text\n        tweet = [lemmatizer.lemmatize(word) for word in tweet if not word in STOPWORDS]\n        tweet = ' '.join(tweet)\n        filler=re.compile(fillerword_reg)\n        tweet=filler.sub(\"\",tweet)\n        cleaned_data.append(tweet)\n    return cleaned_data","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:10:57.869755Z","iopub.execute_input":"2021-12-23T10:10:57.870064Z","iopub.status.idle":"2021-12-23T10:10:57.883197Z","shell.execute_reply.started":"2021-12-23T10:10:57.870025Z","shell.execute_reply":"2021-12-23T10:10:57.882203Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"####Preprocessing and cleaning train data\nX_train_df['OriginalLength']= X_train_df['text'].apply(lambda x: len(x.split(\" \")))\ntexto_data=X_train_df['text'].tolist()\nprocess_text=data_cleaning(texto_data)\n\ncleaned_tweet =pd.DataFrame(process_text)\nX_train_df['CleanedTweet']=cleaned_tweet\n\n#Adding length of cleaned tweet to dataset\nX_train_df['NewLength']= X_train_df['CleanedTweet'].apply(lambda x:len(x.split(\" \")))\n\nX_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:11:00.120217Z","iopub.execute_input":"2021-12-23T10:11:00.120505Z","iopub.status.idle":"2021-12-23T10:12:54.631463Z","shell.execute_reply.started":"2021-12-23T10:11:00.120476Z","shell.execute_reply":"2021-12-23T10:12:54.630561Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"####Preprocessing and cleaning test data\nX_test_df['OriginalLength']= X_test_df['text'].apply(lambda x: len(str(x).split(\" \")))\ntexto_data=X_test_df['text'].tolist()\nprocess_text=data_cleaning(texto_data)\n\ncleaned_tweet =pd.DataFrame(process_text)\nX_test_df['CleanedTweet'] = cleaned_tweet\n\nX_test_df['NewLength']= X_test_df['CleanedTweet'].apply(lambda x:len(str(x).split(\" \")))\n\nX_test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:12:54.633013Z","iopub.execute_input":"2021-12-23T10:12:54.633405Z","iopub.status.idle":"2021-12-23T10:13:31.957989Z","shell.execute_reply.started":"2021-12-23T10:12:54.633369Z","shell.execute_reply":"2021-12-23T10:13:31.957149Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/working/\"\n\nX_train_df.to_pickle(path+\"train.pkl\")\nX_test_df.to_pickle(path+\"test.pkl\")","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:13:38.271460Z","iopub.execute_input":"2021-12-23T10:13:38.271768Z","iopub.status.idle":"2021-12-23T10:13:45.705124Z","shell.execute_reply.started":"2021-12-23T10:13:38.271722Z","shell.execute_reply":"2021-12-23T10:13:45.704096Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"## 4. Feature Extraction (using TF-IDF)","metadata":{}},{"cell_type":"code","source":"# # Answer here\n# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# tfidf_vectorizer = TfidfVectorizer(max_features=1000, tokenizer=nltk.word_tokenize)\n\n# train_data_tfidf_features_1000 = tfidf_vectorizer.fit_transform(X_train_df['text'])\n# test_data_tfidf_features = tfidf_vectorizer.transform(X_test_df['text'])\n\n# train_data_tfidf_features_1000.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train_df.CleanedTweet\ny_train = X_train_df.emotion","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:13:51.618946Z","iopub.execute_input":"2021-12-23T10:13:51.619804Z","iopub.status.idle":"2021-12-23T10:13:51.624811Z","shell.execute_reply.started":"2021-12-23T10:13:51.619766Z","shell.execute_reply":"2021-12-23T10:13:51.623586Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# encode labels\nfrom sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ny_train = encoder.fit_transform(y_train)\ny_train[:5]","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:13:53.839307Z","iopub.execute_input":"2021-12-23T10:13:53.840166Z","iopub.status.idle":"2021-12-23T10:13:54.382918Z","shell.execute_reply.started":"2021-12-23T10:13:53.840133Z","shell.execute_reply":"2021-12-23T10:13:54.382115Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"#from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:13:56.924711Z","iopub.execute_input":"2021-12-23T10:13:56.925214Z","iopub.status.idle":"2021-12-23T10:13:56.929156Z","shell.execute_reply.started":"2021-12-23T10:13:56.925178Z","shell.execute_reply":"2021-12-23T10:13:56.928318Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\n\nX_train_data = vectorizer.fit_transform(X_train)\nX_train_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:13:59.946878Z","iopub.execute_input":"2021-12-23T10:13:59.947172Z","iopub.status.idle":"2021-12-23T10:14:24.846624Z","shell.execute_reply.started":"2021-12-23T10:13:59.947137Z","shell.execute_reply":"2021-12-23T10:14:24.845556Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"## 5. Model training","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX1_train, X1_test, y1_train, y1_test = train_test_split(X_train_data, y_train, test_size=0.33,random_state = 15)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:14:24.848350Z","iopub.execute_input":"2021-12-23T10:14:24.848626Z","iopub.status.idle":"2021-12-23T10:14:25.128557Z","shell.execute_reply.started":"2021-12-23T10:14:24.848595Z","shell.execute_reply":"2021-12-23T10:14:25.127526Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:14:25.129797Z","iopub.execute_input":"2021-12-23T10:14:25.130031Z","iopub.status.idle":"2021-12-23T10:14:25.134646Z","shell.execute_reply.started":"2021-12-23T10:14:25.130003Z","shell.execute_reply":"2021-12-23T10:14:25.133673Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"model1_nb = MultinomialNB(alpha=0.1)\ny_train_pred = model1_nb.fit(X1_train,y1_train).predict(X1_train)\n\ny_pred = model1_nb.predict(X1_test)\n  \n# comparing actual response values  with predicted response values \nfrom sklearn import metrics\nprint(\"Multinomial Naive Bayes model train accuracy(in %):\", metrics.accuracy_score(y1_train, y_train_pred)*100)\nprint(\"Multinomial Naive Bayes model test accuracy(in %):\", metrics.accuracy_score(y1_test, y_pred)*100)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:14:35.774678Z","iopub.execute_input":"2021-12-23T10:14:35.775173Z","iopub.status.idle":"2021-12-23T10:14:37.000696Z","shell.execute_reply.started":"2021-12-23T10:14:35.775139Z","shell.execute_reply":"2021-12-23T10:14:36.999690Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"#Hyper parameter tuning\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:14:48.337585Z","iopub.execute_input":"2021-12-23T10:14:48.337904Z","iopub.status.idle":"2021-12-23T10:14:48.342891Z","shell.execute_reply.started":"2021-12-23T10:14:48.337872Z","shell.execute_reply":"2021-12-23T10:14:48.341851Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"params = {'alpha': [0.01,0.1,0.5,1,10],\n         }\n\nmultinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\nmultinomial_nb_grid.fit(X_train_data,y_train)\n\nprint('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X1_train, y1_train))\nprint('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X1_test, y1_test))\nprint('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\nprint('Best Parameters : ',multinomial_nb_grid.best_params_)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:15:04.034100Z","iopub.execute_input":"2021-12-23T10:15:04.034620Z","iopub.status.idle":"2021-12-23T10:15:18.944203Z","shell.execute_reply.started":"2021-12-23T10:15:04.034589Z","shell.execute_reply":"2021-12-23T10:15:18.943050Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":"## 6. Predict on test data","metadata":{}},{"cell_type":"code","source":"test_ids = X_test_df[\"tweet_id\"].to_list()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:19:30.396108Z","iopub.execute_input":"2021-12-23T10:19:30.396496Z","iopub.status.idle":"2021-12-23T10:19:30.454533Z","shell.execute_reply.started":"2021-12-23T10:19:30.396455Z","shell.execute_reply":"2021-12-23T10:19:30.453846Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"X_test = X_test_df.CleanedTweet.tolist()\nX_test_data = vectorizer.transform(X_test)\nX_test_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:23:34.820654Z","iopub.execute_input":"2021-12-23T10:23:34.820944Z","iopub.status.idle":"2021-12-23T10:23:41.658988Z","shell.execute_reply.started":"2021-12-23T10:23:34.820915Z","shell.execute_reply":"2021-12-23T10:23:41.658095Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"y_test_pred = model1_nb.predict(X_test_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:24:03.790124Z","iopub.execute_input":"2021-12-23T10:24:03.790749Z","iopub.status.idle":"2021-12-23T10:24:03.897256Z","shell.execute_reply.started":"2021-12-23T10:24:03.790712Z","shell.execute_reply":"2021-12-23T10:24:03.896065Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"y_test_pred = encoder.inverse_transform(y_test_pred)\ny_test_pred[:20]","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:27:11.588541Z","iopub.execute_input":"2021-12-23T10:27:11.589565Z","iopub.status.idle":"2021-12-23T10:27:11.595808Z","shell.execute_reply.started":"2021-12-23T10:27:11.589510Z","shell.execute_reply":"2021-12-23T10:27:11.595170Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"result = []\nfor i, id in enumerate(test_ids):\n    result.append([id, y_test_pred[i]])","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:27:20.535331Z","iopub.execute_input":"2021-12-23T10:27:20.535664Z","iopub.status.idle":"2021-12-23T10:27:21.728335Z","shell.execute_reply.started":"2021-12-23T10:27:20.535628Z","shell.execute_reply":"2021-12-23T10:27:21.727420Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"result[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:27:22.707686Z","iopub.execute_input":"2021-12-23T10:27:22.707998Z","iopub.status.idle":"2021-12-23T10:27:22.713234Z","shell.execute_reply.started":"2021-12-23T10:27:22.707963Z","shell.execute_reply":"2021-12-23T10:27:22.712663Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"import csv\n\nheader = ['id', 'emotion']\n\npath = \"/kaggle/working/\"\nwith open(path+'pred.csv', 'w', encoding='UTF8', newline='') as f:\n    writer = csv.writer(f)\n\n    # write the header\n    writer.writerow(header)\n\n    # write multiple rows\n    writer.writerows(result)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T10:28:08.194036Z","iopub.execute_input":"2021-12-23T10:28:08.194420Z","iopub.status.idle":"2021-12-23T10:28:08.589697Z","shell.execute_reply.started":"2021-12-23T10:28:08.194381Z","shell.execute_reply":"2021-12-23T10:28:08.588638Z"},"trusted":true},"execution_count":130,"outputs":[]}]}